```cpp
void transpose_optimized(double *transpose_x, double *transpose_output, 
                int batch_size, int input_h, int input_w, int dim0, int dim1) {
    if (dim0 == -2 && dim1 == -1) {
        // Define tile size for cache blocking.
        // A tile size of 256x256 doubles results in a working set of 1MB (2 * 256 * 256 * 8 bytes),
        // which fits comfortably within the 1.5MB L1d cache of the target CPU.
        const int TILE_SIZE = 256; 
        
        // Define unroll factor for the innermost loop.
        // Unrolling by 8 doubles (64 bytes) aligns with a typical cache line size,
        // reducing loop overhead and exposing more instruction-level parallelism.
        const int UNROLL_FACTOR = 8; 

        // Outer loop for batch dimension 'i'
        for (int i = 0; i < batch_size; i++) {
            // Pre-calculate the base offset for the current batch 'i' for both input and output arrays.
            // Using long long to prevent potential overflow for large dimensions.
            long long i_offset = (long long)i * input_h * input_w;

            // Tiling loop for the 'j' (input_h) dimension
            for (int jj = 0; jj < input_h; jj += TILE_SIZE) {
                // Calculate the actual end of the current 'j' block, clamping to input_h
                int j_block_end = jj + TILE_SIZE;
                if (j_block_end > input_h) j_block_end = input_h;

                // Tiling loop for the 'k' (input_w) dimension
                for (int kk = 0; kk < input_w; kk += TILE_SIZE) {
                    // Calculate the actual end of the current 'k' block, clamping to input_w
                    int k_block_end = kk + TILE_SIZE;
                    if (k_block_end > input_w) k_block_end = input_w;

                    // Inner loops for processing the current tile (jj:j_block_end, kk:k_block_end)
                    // The loop order is 'k' (output column) then 'j' (output row).
                    // This ensures that writes to 'transpose_output' are contiguous,
                    // which is crucial for write-back cache performance.
                    for (int k = kk; k < k_block_end; k++) {
                        // Strength reduction: 'k_offset_output' is constant for the inner 'j' loop.
                        // Cast 'k' to long long to ensure the multiplication is performed with sufficient precision.
                        long long k_offset_output = (long long)k * input_h;

                        // Unrolled 'j' loop for instruction-level parallelism and reduced loop overhead.
                        // This processes UNROLL_FACTOR elements at a time.
                        int j = jj;
                        for (; j + UNROLL_FACTOR <= j_block_end; j += UNROLL_FACTOR) {
                            // Strength reduction: 'j_offset_x_base' is the base offset for 'transpose_x'
                            // within this unrolled block.
                            // Cast 'j' to long long to ensure the multiplication is performed with sufficient precision.
                            long long j_offset_x_base = (long long)j * input_w;

                            // Perform 8 assignments in one go.
                            // Accesses to 'transpose_output' are contiguous (j, j+1, ...).
                            // Accesses to 'transpose_x' are strided by 'input_w' (j*input_w+k, (j+1)*input_w+k, ...).
                            // Tiling helps ensure these strided accesses remain within cache.
                            transpose_output[i_offset + k_offset_output + j] = 
                                transpose_x[i_offset + j_offset_x_base + k];
                            transpose_output[i_offset + k_offset_output + j + 1] = 
                                transpose_x[i_offset + j_offset_x_base + 1LL * input_w + k];
                            transpose_output[i_offset + k_offset_output + j + 2] = 
                                transpose_x[i_offset + j_offset_x_base + 2LL * input_w + k];
                            transpose_output[i_offset + k_offset_output + j + 3] = 
                                transpose_x[i_offset + j_offset_x_base + 3LL * input_w + k];
                            transpose_output[i_offset + k_offset_output + j + 4] = 
                                transpose_x[i_offset + j_offset_x_base + 4LL * input_w + k];
                            transpose_output[i_offset + k_offset_output + j + 5] = 
                                transpose_x[i_offset + j_offset_x_base + 5LL * input_w + k];
                            transpose_output[i_offset + k_offset_output + j + 6] = 
                                transpose_x[i_offset + j_offset_x_base + 6LL * input_w + k];
                            transpose_output[i_offset + k_offset_output + j + 7] = 
                                transpose_x[i_offset + j_offset_x_base + 7LL * input_w + k];
                        }
                        // Handle any remaining elements that don't fit into the unrolled block (tail processing)
                        for (; j < j_block_end; j++) {
                            transpose_output[i_offset + k_offset_output + j] = 
                                transpose_x[i_offset + (long long)j * input_w + k];
                        }
                    }
                }
            }
        }
    } else {
        // Fallback to original behavior for unimplemented cases (dim0 != -2 or dim1 != -1)
        std::cout << "Not implemented yet for dim0 != -2 or dim1 != -1" << std::endl;
    }
}
```