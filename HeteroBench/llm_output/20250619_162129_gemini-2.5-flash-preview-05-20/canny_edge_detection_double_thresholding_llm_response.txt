```cpp
void double_thresholding_optimized(double *suppressed_image, int height,
                                   int width, int high_threshold,
                                   int low_threshold, uint8_t *outImage) {
  // Convert thresholds to double once to avoid repeated implicit conversions
  // from int to double inside the loop. This is a minor strength reduction.
  const double high_thresh_d = static_cast<double>(high_threshold);
  const double low_thresh_d = static_cast<double>(low_threshold);

  // Define an unroll factor for the inner loop.
  // Unrolling helps reduce loop overhead (branching, counter increments) and
  // exposes more instruction-level parallelism (ILP) by allowing the CPU to
  // fetch and execute multiple independent operations concurrently.
  const int UNROLL_FACTOR = 4; // A common factor, balancing register pressure and ILP.

  // Loop Transformation: Interchange loops for significantly better cache locality.
  // The original code iterated column-major (col outer, row inner, pxIndex = col + row*width),
  // which results in strided memory access for row-major stored arrays.
  // By making 'row' the outer loop and 'col' the inner loop, and calculating
  // pxIndex as row*width + col, we achieve contiguous memory access,
  // which is crucial for performance on modern CPUs due to cache efficiency.
  for (int row = 0; row < height; ++row) {
    // Strength Reduction: Calculate the base offset for the current row once.
    // This avoids repeated multiplication (row * width) inside the inner loop.
    int row_offset = row * width;

    // Process the inner loop with unrolling.
    int col = 0;
    for (; col <= width - UNROLL_FACTOR; col += UNROLL_FACTOR) {
      // Calculate indices for unrolled iterations.
      // These indices will be contiguous in memory, leveraging cache lines.
      int pxIndex0 = row_offset + col;
      int pxIndex1 = row_offset + col + 1;
      int pxIndex2 = row_offset + col + 2;
      int pxIndex3 = row_offset + col + 3;

      // Load values for unrolled iterations.
      // These memory loads are independent and can potentially be executed
      // in parallel by the CPU's out-of-order execution engine.
      double val0 = suppressed_image[pxIndex0];
      double val1 = suppressed_image[pxIndex1];
      double val2 = suppressed_image[pxIndex2];
      double val3 = suppressed_image[pxIndex3];

      // Process iteration 0: Apply the double thresholding logic.
      // Modern compilers and CPUs are efficient at handling these conditional branches,
      // potentially using branch prediction or conditional move instructions.
      if (val0 > high_thresh_d) {
        outImage[pxIndex0] = 255;
      } else if (val0 > low_thresh_d) {
        outImage[pxIndex0] = 100;
      } else {
        outImage[pxIndex0] = 0;
      }

      // Process iteration 1
      if (val1 > high_thresh_d) {
        outImage[pxIndex1] = 255;
      } else if (val1 > low_thresh_d) {
        outImage[pxIndex1] = 100;
      } else {
        outImage[pxIndex1] = 0;
      }

      // Process iteration 2
      if (val2 > high_thresh_d) {
        outImage[pxIndex2] = 255;
      } else if (val2 > low_thresh_d) {
        outImage[pxIndex2] = 100;
      } else {
        outImage[pxIndex2] = 0;
      }

      // Process iteration 3
      if (val3 > high_thresh_d) {
        outImage[pxIndex3] = 255;
      } else if (val3 > low_thresh_d) {
        outImage[pxIndex3] = 100;
      } else {
        outImage[pxIndex3] = 0;
      }
    }

    // Handle the remaining elements (if 'width' is not a multiple of UNROLL_FACTOR).
    // This ensures functional equivalence for all input image dimensions.
    for (; col < width; ++col) {
      int pxIndex = row_offset + col;
      if (suppressed_image[pxIndex] > high_thresh_d) {
        outImage[pxIndex] = 255;
      } else if (suppressed_image[pxIndex] > low_thresh_d) {
        outImage[pxIndex] = 100;
      } else {
        outImage[pxIndex] = 0;
      }
    }
  }
}
```