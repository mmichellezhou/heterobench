```cpp
void conv2d_optimized(double *conv2d_input, double *conv2d_kernel, double *input_padded, double conv2d_bias, int stride, int padding, int input_h, int input_w, int kernel_h, int kernel_w, double *conv2d_output)
{
  // The pad_input function is assumed to be defined elsewhere and correctly populates input_padded.
  // It is called once at the beginning as per the original code structure.
  pad_input(conv2d_input, input_padded, input_h, input_w, padding);

  // Pre-calculate output dimensions
  int output_h = (input_h + 2 * padding - kernel_h) / stride + 1;
  int output_w = (input_w + 2 * padding - kernel_w) / stride + 1;

  // Pre-calculate padded input width, which is constant for all accesses
  // This is a form of strength reduction, moving a constant calculation out of loops.
  int padded_input_width = input_w + 2 * padding;

  // Outer loops for output dimensions
  for (int i = 0; i < output_h; i++) {
    // Pre-calculate the base row start index for the input_padded array for the current 'i'
    // This avoids re-calculating 'i * stride' in the inner loops.
    int base_input_row_start = i * stride;

    for (int j = 0; j < output_w; j++) {
      // Pre-calculate the base column start index for the input_padded array for the current 'j'
      // This avoids re-calculating 'j * stride' in the inner loops.
      int base_input_col_start = j * stride;

      // Use multiple accumulators to improve Instruction-Level Parallelism (ILP).
      // This breaks the data dependency chain of a single 'tmp' variable, allowing
      // the CPU to execute multiple multiply-add operations concurrently.
      double tmp0 = 0.0;
      double tmp1 = 0.0;
      double tmp2 = 0.0;
      double tmp3 = 0.0;

      // Loop over kernel height
      for (int k = 0; k < kernel_h; k++) {
        // Pre-calculate the row offset for input_padded for the current 'k'
        // This avoids re-calculating '(base_input_row_start + k) * padded_input_width' in the innermost loop.
        int input_row_offset = (base_input_row_start + k) * padded_input_width;
        // Pre-calculate the row offset for conv2d_kernel for the current 'k'
        // This avoids re-calculating 'k * kernel_w' in the innermost loop.
        int kernel_row_offset = k * kernel_w;

        // Loop over kernel width
        int l = 0;
        // Unroll the innermost 'l' loop by a factor of 4.
        // This reduces loop overhead (branching, index incrementing) and exposes more ILP.
        // Each unrolled iteration uses a different accumulator (tmp0, tmp1, tmp2, tmp3)
        // to maximize independent operations.
        for (; l + 3 < kernel_w; l += 4) {
          tmp0 += input_padded[input_row_offset + base_input_col_start + l]     * conv2d_kernel[kernel_row_offset + l];
          tmp1 += input_padded[input_row_offset + base_input_col_start + l + 1] * conv2d_kernel[kernel_row_offset + l + 1];
          tmp2 += input_padded[input_row_offset + base_input_col_start + l + 2] * conv2d_kernel[kernel_row_offset + l + 2];
          tmp3 += input_padded[input_row_offset + base_input_col_start + l + 3] * conv2d_kernel[kernel_row_offset + l + 3];
        }

        // Handle remaining elements if kernel_w is not a multiple of 4
        // The remainder elements are added to the first accumulator (tmp0).
        for (; l < kernel_w; l++) {
          tmp0 += input_padded[input_row_offset + base_input_col_start + l] * conv2d_kernel[kernel_row_offset + l];
        }
      }

      // Sum up the results from all accumulators to get the final 'tmp' value
      double tmp = tmp0 + tmp1 + tmp2 + tmp3;

      // Store the final convolution result for the current output pixel
      conv2d_output[i * output_w + j] = tmp + conv2d_bias;
    }
  }
}
```